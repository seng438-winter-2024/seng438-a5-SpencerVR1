**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:  1    |   |
|-----------------|---|
| Student Names:  | Spencer van Roessel |
|                 | Harris Hasnain |
|                 | Houssem Zaggar |
|                 | Kaylyn Tanton |

# Introduction

This lab report explores two primary methodologies: Reliability Growth Testing in Part 1 and Reliability Assessment using Reliability Demonstration Charts (RDC) in  Part 2. 

Part 1 gave us hands-on experience in assessing the reliability of a hypothetical system given its failure data collected during integration testing. This was done using the reliability growth assessment tool, C-SFRAT. Through this, we better understood reliability growth testing, learned how to measure the failure rate, MTTF, and reliability of the SUT, and became familiar with C-SFRAT. 

Part 2 involved using a Reliability Demonstration Chart (RDC) to check whether or not the target failure rate or MTTF is met. We had to decide upon the adequacy of testing for a given MTTF of the SUT by plotting the test data. This section allowed us to become familiar with the features and usage of an RDCtool.

# Assessment Using Reliability Growth Testing 

## Result of model comparison (selecting top two models)	 

<img width="959" alt="MVF all" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/9a6db9ac-5a52-4cdb-a12d-6213a30d8c74">

We overlayed all the models over each other and deselected the worst ones until we were left with the best two. We found that the best models were 'TL' and 'IFRGSB'. The model comparison tab confirmed this, as those two models exhibited the lowest log-likelihood scores which is a metric that indicates its goodness of fit.

<img width="959" alt="MVF 2" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/0ad8bfde-7a4b-44dd-a6ab-07b8bd3367b0">

## Result of range analysis (an explanation of which part of data is good for proceeding with the analysis)	

## Plots for failure rate and reliability of the SUT for the test data provided	

MVF Graph of 'IFRGSB' Model
<img width="959" alt="MVF orange" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/96232f05-f131-4d2e-9973-42d20b4ecd4f">

MVF Graph of 'TL' Model
<img width="959" alt="MVF grey" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/9ddb0d0c-3b61-4fbd-9f97-aa55e2f6f10c">

Intensity Graph of 'IFRGSB' Model
<img width="959" alt="intensity orange" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/22453a4d-4ed5-4bcc-ac9e-1cc03588c4c9">

Intensity Graph of 'TL' Model
<img width="959" alt="intensity grey" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/113068550/9cd016f3-4149-4b01-a91e-0dc000734d37">

## A discussion on decision making given a target failure rate

We used the C-SFRAT reliability assessment tool to plot the varying amount of failures over time intervals from our dataset, then used these results to analyze the change in failure rate and associated reliability over time for our system. We determined a target value for our failure rate by dividing the total number of failures in the interval by the time length of that interval. The plot of failure rate and amount of failures changing over time can help us make a decision as to whether we are satisfied or not with the reliability of our system. If the target failure rate is found to be too high, we can bring it down by improving the quality of our code and depth of our test suite to make sure that we are thoroughly assessing our code as effectively as possible before release to give end-users the highest quality experience, hence the primary objective during this failure rate analysis and decision making process should be to try to lower the failure rate as much as possible.

## A discussion on the advantages and disadvantages of reliability growth analysis	

Advantages:
- Can be used to predict the system's reliability  over time.
- Can help teams allocate resources better or prioritize more crucial improvements.
- Can help identify weaknesses in the system earlier in the development process, addressing issues promptly and improving overall reliability.
- Promotes continuous improvement by tracking metrics over time.
- Can help predict maintenance needs to prevent failures before they happen.
- Saves cost by reducing downtime.
- Can help determine which tests need to be run and when they need to run.

Disadvantages:
- Actual results may differ from predictions. For example, unexpected failures or changes in the environment or system will affect predictive accuracy.
- Dependent on the data provided. The predictions will be negatively impacted if the data is insufficient, inaccurate, or difficult to obtain.
- Can be resource intensive. Smaller organizations may struggle with time, data collection, and analysis.
- Translating findings into actionable improvements may be challenging. 
- Can be complex and may require technical expertise.

# Assessment Using Reliability Demonstration Chart 

## 3 plots for MTTFmin, twice and half of it for your test data	
MTTFmin:

<img width="721" alt="Screenshot 2024-04-05 at 12 31 59 PM" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/114002342/e85b5e77-43c0-42bb-ac04-6404704d273b">

MTTF twice:

<img width="720" alt="Screenshot 2024-04-05 at 12 36 48 PM" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/114002342/1da70796-80e8-42c2-b6e2-8c6a03b5814b">


MTTF half:

<img width="716" alt="Screenshot 2024-04-05 at 12 37 31 PM" src="https://github.com/seng438-winter-2024/seng438-a5-SpencerVR1/assets/114002342/de71bfb5-1f2c-439d-884b-c971640f659f">


## Explain your evaluation and justification of how you decide the MTTFmin	
We used the RDC tool for the second part of the lab. The MTTFmin was selected for the dataset through trial and error. First we calculated the MTTF for the current dataset and adjusted the failure intensity objective through the max acceptable number of failures per number of input events. We then altered those values until the observed failure line was just barely entering the accept region. The line touches the green region briefly before going back into the continue part, then enters the green region at the end of the graph. Decreasing the MTTFmin by one unit put the failure line less in the accept region, but also moved a separate part of it up enough that it overlapped with the reject boundary, making that MTTFmin value too low. 

## A discussion on the advantages and disadvantages of RDC	

Advantages:
- Allows for a quick assessment of reliability performance.
- Offers a clear visual representation of reliability trends over time, making it easier to interpret.
- Helps make informed decisions, such as whether to release a product or continue testing.
- Helps identify reliability trends and patterns.

Disadvantages:
- Does not allow reliability analysis based on failure numbers over a set time interval
- We had to use a dataset from the Time_Between_Failures folder

# Comparison of Results

For Reliability Growth Testing, we used a dataset that included failure counts to plot the number of failures over time intervals. This provides a good view of the overall reliability of the system over a given period of time. We picked the two best-fitting models that had the lowest log-likelihood scores. These models were 'TL' and 'IFRGSB'. 

For Reliability Demonstration Chart, we needed to pick a dataset from the 'Time Between Failures' set, as RDC is only based on inter-failure times and mean time to failure (MTTF). We picked a MTTFmin of 25 seconds. As stated above, this was found through trial and error, until the observed failure line fell just into the accept region. Further testing was performed with twice the MTTFmin (50 seconds) and half the MTTFmin (12.5 seconds).

# Discussion on Similarity and Differences of the Two Techniques

### Similarities:
- Both Reliability Growth Testing (RGT) and Reliability Demonstration Chart (RDC) are used for assessing the reliability of software systems. 
- Both techniques involve statistical analysis of data to come to a conclusion on the reliability of a system.
- Target failure rate or mean time to failure (MTTF) are necessary pieces of data for both RGT and RDC.

### Differences:
- The primary difference lies in their timing within the product development lifecycle. RGT aims to identify reliability issues during the development phase, while RDC focuses on demonstrating that the system meets reliability requirements, such as target mean time to failure or failure rates, after the development phase and before deployment.
- RDC is only based on inter-failure times and target failure rate, or MTTF. However, RGT can use a wider range of input data. It can be based on interfailure times, as well as failure count.
- Due to these differences, we used a time between failures data set for RDC and a failure count data set for RGT.

# How the team work/effort was divided and managed

For part 1, we all worked together on one laptop because it was the only one compatible with the software we had to use. The programming was carried out collaboratively in person. Part 2 was compatible with all of our systems so it was easier to split up once we started it together. The lab report was split evenly. 

# Difficulties encountered, challenges overcome, and lessons learned

The biggest difficulty we encountered was the software's incompatibility with MacOS. All of our team members had Macs besides one that had a spare Windows laptop. This made it harder to collaborate on the lab together. To overcome this, most of the work was done synchronously in person. 

# Comments/feedback on the lab itself

Having a broader selection of software tools that are compatible with various systems would have significantly enhanced the quality of this lab. Additionally, providing more detailed instructions or a demonstration of the software would have eased the technical difficulties, allowing us to focus more on practicing the core concepts.
